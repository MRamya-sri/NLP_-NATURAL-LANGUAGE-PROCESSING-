# -*- coding: utf-8 -*-
"""NLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p1ChT84cZEU2mTaDmsarjI8QOSpqTzgt

**1. TOKENIZATION**
"""

import os
import nltk
import nltk.corpus
nltk.download('punkt')

AI = " Artificial intelligence (AI) refers to technology that enables computers and machines to simulate human intelligence and problem-solving capabilities. It allows them to perform tasks that would otherwise require human intervention or intelligence. Examples of AI applications include digital assistants, GPS guidance, autonomous vehicles, and generative AI tools like OpenAIâ€™s ChatGPT1. AI encompasses fields such as machine learning and deep learning, which involve developing algorithms inspired by human decision-making processes. These algorithms learn from available data and make increasingly accurate predictions over time. There are two main types of AI"

type(AI)

"""to get tokens need to import word_tokenize"""

from nltk.tokenize import word_tokenize

AI_tokens = word_tokenize(AI)
AI_tokens

len(AI_tokens)

"""frequency distribution (**FreqDist**) , returns the often number of words count"""

from nltk.probability import FreqDist
fdist = FreqDist(AI_tokens)
fdist

"""particular word frequeny"""

fdist['intelligence']

len(fdist)  #unique number

"""most common top 10 words"""

fdist_top10 = fdist.most_common(10)
fdist_top10

"""removing all blankline's

"""

from nltk.tokenize import blankline_tokenize
AI_blank = blankline_tokenize(AI)
AI_blank

len(AI_blank)

"""BIGRAMS, TRIGRAMS, NGRAMS"""

from nltk.util import bigrams, trigrams, ngrams

grams = list(nltk.bigrams(AI_tokens))
grams

grams1 = list(nltk.trigrams(AI_tokens))
grams1

gram_N = list(nltk.ngrams(AI_tokens, 5))
gram_N

"""**2. STEMMING**"""

from nltk.stem import PorterStemmer
pst = PorterStemmer()

pst.stem("having")

from nltk.stem import LancasterStemmer
lst = LancasterStemmer()

lst.stem("having")

from nltk.stem import SnowballStemmer
sst = SnowballStemmer('english')

sst.stem("having")

"""**3. LEMMATIZATION**

"""

nltk.download('wordnet')
from nltk.stem import wordnet
from nltk.stem import WordNetLemmatizer
wl = WordNetLemmatizer()

wl.lemmatize("corpora")

"""**STOP WORDS REMOVING**

"""

nltk.download('stopwords')
from nltk.corpus import stopwords
stopwords.words('english')

len(stopwords.words('english'))

"""**removing all special characters**"""

import re

punctuation = re.compile(r'[-.?!,:;()|0-9]')

punctuation

post_punctuation = []
for token in AI_tokens:
  word = punctuation.sub("", token)
  if len(word) > 0:
    post_punctuation.append(word)

post_punctuation

"""**Parts of speech**"""

text = "If you want to search for a specific word, this is quite easy. Just know that if any of your documents contain this word or phrase, then all of them will show up at once. To do this, click within the search box after opening the file."
text_token = word_tokenize(text)
text_token

nltk.download('averaged_perceptron_tagger')

for token in text_token:
  print(nltk.pos_tag([token]))

"""**NAMED ENTITY RECOGNITION (NER)**"""

nltk.download('words')
nltk.download('maxent_ne_chunker')
from nltk import ne_chunk

text = "If you want to search for a specific word, this is quite easy. Just know that if any of your documents contain this word or phrase, then all of them will show up at once. To do this, click within the search box after opening the file."

tokens = word_tokenize(text) #tokens
tags = nltk.pos_tag(tokens)  #tag pos of tokens

ner = ne_chunk(tags)
ner